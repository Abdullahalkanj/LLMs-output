# Evaluation of Prompt Design Strategies in LLMs for Software Maintenance Tasks

## Overview

This repository presents an evaluation of three state-of-the-art large language models (LLMs) on two essential software maintenance tasks:

- **Bug Detection and Correction**
- **Code Explanation**

The evaluation investigates how different **prompt design strategies** affect the output quality of the LLMs. The three prompt types tested are:

- **Basic Prompt**
- **Structured Prompt**
- **Chain-of-Thought (CoT) Prompt**

## Evaluated Language Models

The models evaluated in this study are:

- **ChatGPT-4o**
- **ChatGPT-4o mini**
- **Claude**

Each model was given the same task and code input, but with differently structured prompts depending on the strategy being tested.

## Prompt Design Strategies

### Bug Detection and Correction Prompts

- **Basic Prompt**:  
  `"Can you find and fix the bug in this code?"`  
  `+ [code]`

- **Structured Prompt**:  
  `"You are an expert software engineer. Analyze the code below, explain the bug, and provide a fix."`  
  `+ [code]`

- **Chain-of-Thought Prompt**:  
  `"Explain your reasoning step by step before suggesting a fix for the code below."`  
  `+ [code]`

### Code Explanation Prompts

- **Basic Prompt**:  
  `"Explain what this code does."`  
  `+ [code]`

- **Structured Prompt**:  
  `"You are a senior developer. Describe the functionality in comments and by words, input, and output of this code."`  
  `+ [code]`

- **Chain-of-Thought Prompt**:  
  `"Step-by-step explain the purpose of the code, then go line by line explaining what it does."`  
  `+ [code]`

## Repository Structure

The repository is organized into two main directories corresponding to the evaluated tasks:

### 1. `bug_detection_and_correction/`

- Contains 10 subdirectories named `1/` through `10/`, each representing a distinct test case.
- Each test case directory includes:
  - `correct_code/`: The correct version of the code.
  - `buggy_code/`: The buggy version of the code provided to the LLMs.
  - `GPT4oRsp/`: Response from ChatGPT-4o.
  - `Gpto4miniRsp/`: Response from ChatGPT-4o mini.
  - `ClaudeRsp/`: Response from Claude.

### 2. `code_explanation/`

- Contains 8 subdirectories named `1/` through `8/`, each representing a distinct code snippet.
- Each code snippet directory includes:
  - `correct_code/`: The code provided to the LLMs for explanation.
  - `GPT4oRsp/`: Response from ChatGPT-4o.
  - `Gpto4miniRsp/`: Response from ChatGPT-4o mini.
  - `ClaudeRsp/`: Response from Claude.

Each response directory contains the output generated by the respective LLM when prompted with the corresponding prompt design strategy.

## Purpose

This evaluation is designed to:

- Understand how different prompt strategies influence the performance of LLMs in technical problem-solving.
- Identify which models and strategies are more reliable for tasks such as debugging and code understanding.
- Inform better prompt engineering for practical use in software development and maintenance.

## License

This project is intended for educational and research purposes only.
Created and done with Zaid Alajlani and Abdullah Alkanj